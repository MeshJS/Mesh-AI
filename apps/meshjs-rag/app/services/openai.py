from openai import AsyncOpenAI
from typing import List
from tenacity import retry, wait_random_exponential, stop_after_attempt
import json

DOCUMENT_CONTEXT_PROMPT = """
<document>
{doc_content}
</document>
"""

CHUNK_CONTEXT_PROMPT = """
Here is the chunk we want to situate within the whole document
<chunk>
{chunk_content}
</chunk>

Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
Answer only with the succinct context and nothing else.
"""

AI_PROMPT = """
You are a MeshJS expert assistant. Help developers with MeshJS questions using the provided context. Use the documentation context to answer questions about MeshJS and Cardano development. Provide accurate code examples and explanations based on the context provided.

When answering:
- Give direct, helpful answers based on the context
- Include relevant code examples when available
- Explain concepts clearly for developers
- Include any documentation or resource links found in the context. Handle them as follows:
  - When you find "location: ...", include the link in your answer as a reference.
  - If multiple links are present, include all relevant ones.
- If the context doesn't cover the question, say so clearly.
- Do not invent or assume APIs, methods, or functionality not in the documentation.

Be concise but thorough. Focus on practical, actionable guidance for MeshJS development.
"""

class OpenAIService:
  def __init__(self, embedding_api_key: str, completion_api_key: str, completion_model: str, base_url: str = None):
    self.embedding_client = AsyncOpenAI(api_key=embedding_api_key)
    self.completion_client = AsyncOpenAI(
      api_key=completion_api_key,
      base_url=base_url
    )
    self.model = completion_model

  @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
  async def _chat(self, messages, temperature=0.0, max_tokens=None, prompt_cache_key=None, stream: bool = False):
    kwargs = {
      "model": self.model,
      "messages": messages,
      "temperature": temperature,
      "stream": stream
    }

    if prompt_cache_key:
      kwargs["prompt_cache_key"] = prompt_cache_key
    if max_tokens:
      kwargs["max_tokens"] = max_tokens

    return await self.completion_client.chat.completions.create(**kwargs)

  async def situate_context(self, doc: str, chunk: str, cache_key: str) -> str:
    messages = [
      {
        "role": "user",
        "content": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc)
      },
      {
        "role": "user",
        "content": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk)
      }
    ]

    response = await self._chat(messages=messages, max_tokens=1024, prompt_cache_key=cache_key)
    return response.choices[0].message.content

  @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), reraise=True)
  async def get_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
    response = await self.embedding_client.embeddings.create(
      model="text-embedding-3-small",
      input=texts,
      encoding_format="float"
    )

    return [data.embedding for data in response.data]

  @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), reraise=True)
  async def embed_query(self, text: str) -> List[float]:
    response = await self.embedding_client.embeddings.create(
      model="text-embedding-3-small",
      input=text,
      encoding_format="float"
    )

    return response.data[0].embedding

  @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), reraise=True)
  async def get_answer(self, question: str, context: str):
    messages = [
      {
        "role": "system",
        "content": AI_PROMPT
      },
      {
        "role": "user",
        "content": f"""Context: {context}\n\nQuestion: {question}""",
      }
    ]

    stream = await self._chat(messages=messages, stream=True)

    async for chunk in stream:
      yield f"data: {json.dumps(chunk.model_dump())}\n\n"

    yield "data: [DONE]\n\n"

  @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), reraise=True)
  async def get_mcp_answer(self, question: str, context: str):
    messages = [
      {
        "role": "system",
        "content": AI_PROMPT
      },
      {
        "role": "user",
        "content": f"""Context: {context}\n\nQuestion: {question}""",
      }
    ]

    response = await self._chat(messages=messages)
    return response.choices[0].message.content